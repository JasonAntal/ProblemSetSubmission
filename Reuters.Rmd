---
title: "Reuters"
author: "Jason Antal"
date: "2024-08-18"
output: pdf_document
---

```{r setup, include=FALSE #CHANGE FILEPATH AND RUN THIS BEFORE RUNNING BELOW}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tm)
library(wordcloud)
library(topicmodels)
setwd("C:/Users/jason/downloads") #CHANGE FILEPATH HERE
train_file_list = Sys.glob('C50train/*/*.txt')
test_file_list = Sys.glob('C50test/*/*.txt')
#*QUESTION: 
#*What was the most written-about topic? 

file_contents <- readLines("C50train/JonathanBirt/340119newsML.txt")
cat(file_contents, sep = "\n")
```

Revisit the Reuters C50 text corpus that we briefly explored in class. Your task is simple: tell an interesting story, anchored in some analytical tools we have learned in this class, using this data. For example:

    you could cluster authors or documents and tell a story about what you find.
    you could look for common factors using PCA.
    you could train a predictive model and assess its accuracy, constructing features for each document that maximize performance.
    you could do anything else that strikes you as interesting with this data.

Describe clearly what question you are trying to answer, what models you are using, how you pre-processed the data, and so forth. Make sure you include at least one really interesting plot (although more than one might be necessary, depending on your question and approach.)

Format your write-up in the following sections, some of which might be quite short:

    Question: What question(s) are you trying to answer?
    Approach: What approach/statistical tool did you use to answer the questions?
    Results: What evidence/results did your approach provide to answer the questions? (E.g. any numbers, tables, figures as appropriate.)
    Conclusion: What are your conclusions about your questions? Provide a written interpretation of your results, understandable to stakeholders who might plausibly take an interest in this data set.

Regarding the data itself: In the C50train directory, you have 50 articles from each of 50 different authors (one author per directory). Then in the C50test directory, you have another 50 articles from each of those same 50 authors (again, one author per directory). This train/test split is obviously intended for building predictive models, but to repeat, you need not do that on this problem. You can tell any story you want using any methods you want. Just make it compelling!

Note: if you try to build a predictive model, you will need to figure out a way to deal with words in the test set that you never saw in the training set. This is a nontrivial aspect of the modeling exercise. (E.g. you might simply ignore those new words.)

This question will be graded according to three criteria:

    the overall "interesting-ness" of your question and analysis.
    the clarity of your description. We will be asking ourselves: could your analysis be reproduced by a competent data scientist based on what you've said? (That's good.) Or would that person have to wade into the code in order to understand what, precisely, you've done? (That's bad.)
    technical correctness (i.e. did you make any mistakes in execution or interpretation?)

```{r }

clean_text <- function(text) {
    text <- tolower(text)
  # Remove punctuation
  #text <- gsub("[[:punct:]]", " ", text)
  # Remove numbers
  #text <- gsub("[[:digit:]]", "", text)
  # Remove extra whitespace
  #text <- gsub("\\s+", " ", text)
  #text <- trimws(text)
  return(text)
}

# Function to read and process files
read_and_process <- function(file_list) {
  corpus <- Corpus(VectorSource(sapply(file_list, function(file) {
    tryCatch({
      text <- readLines(file, warn = FALSE)
      #text <- paste(text, collapse = " ")
      clean_text(text)
    },
    )
  })))
  
  # Remove NULL entries (failed reads)
  #corpus <- corpus[!sapply(corpus, is.null)]
  
  # Text processing
  corpus <- tm_map(corpus, content_transformer(tolower))
  print(corpus)
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  print(corpus)
  corpus <- tm_map(corpus, stripWhitespace)
  print(corpus)
  return(corpus)
}

# Update file list creation I'VE COMMENTED OUT ALL TEST FILES FOR TROUBLESHOOTING
train_file_list <- list.files(path = "C50train", pattern = "\\.txt$", recursive = TRUE, full.names = TRUE)
#test_file_list <- list.files(path = "C50test", pattern = "\\.txt$", recursive = TRUE, full.names = TRUE)

# Read and process files
train_corpus <- read_and_process(train_file_list)
#test_corpus <- read_and_process(test_file_list)


# Combine train and test corpora
#combined_corpus <- c(train_corpus, test_corpus)

# Create document-term matrix #UPDATE TRAIN_CORPUS BELOW ONCE DONE W TROUBLESHOOTING
dtm <- DocumentTermMatrix(train_corpus)

# Remove sparse terms
#dtm <- removeSparseTerms(dtm, 0.995)

# Calculate term frequencies
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)

# Print diagnostic information DELETE THIS LATER
cat("Number of documents in corpus:", nrow(dtm), "\n")
cat("Total number of terms in DTM:", ncol(dtm), "\n")
cat("Number of non-zero entries in DTM:", sum(slam::col_sums(dtm) > 0), "\n")
cat("Top 20 terms:\n")
print(head(term_freq, 20))

# Get top 20 terms
top_terms <- head(term_freq, 20)

# Create a data frame for plotting
plot_data <- data.frame(term = names(top_terms), frequency = top_terms)

# Create bar plot
ggplot(plot_data, aes(x = reorder(term, frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Most Frequent Terms",
       x = "Terms",
       y = "Frequency") +
  theme_minimal()

# Save the plot
ggsave("top_20_terms.png", width = 10, height = 8)

# Create word cloud
png("wordcloud.png", width = 800, height = 600)
wordcloud(words = names(term_freq), freq = term_freq, min.freq = 50,
          max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
dev.off()

# Print top 10 terms
print(head(term_freq, 10))

# Perform basic topic modeling using LDA
k <- 5  # number of topics
lda_model <- LDA(dtm, k = k)
lda_terms <- terms(lda_model, 10)
print(lda_terms)


#


```


```{r pressure, echo=FALSE}
#CLAUDE GAVE THIS SOLUTION
#
#
#
#
#
read_and_process <- function(file_list) {
  corpus <- Corpus(VectorSource(sapply(file_list, function(file) {
    text <- readLines(file, warn = FALSE)
    paste(text, collapse = " ")
  })))
  
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}

train_file_list <- list.files(path = "C50train", pattern = "\\.txt$", recursive = TRUE, full.names = TRUE)
train_corpus <- read_and_process(train_file_list)
dtm <- DocumentTermMatrix(train_corpus)
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)

cat("Number of documents in corpus:", nrow(dtm), "\n")
cat("Total number of terms in DTM:", ncol(dtm), "\n")
cat("Number of non-zero entries in DTM:", sum(slam::col_sums(dtm) > 0), "\n")
cat("Top 20 terms:\n")
print(head(term_freq, 20))

top_terms <- head(term_freq, 20)
print(top_terms)
plot_data <- data.frame(term = names(top_terms), frequency = top_terms)

ggplot(plot_data, aes(x = reorder(term, frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Most Frequent Terms",
       x = "Terms",
       y = "Frequency")

ggsave("top_20_terms.png", width = 10, height = 8)

png("wordcloud.png", width = 800, height = 600)
wordcloud(words = names(term_freq), freq = term_freq, min.freq = 50,
          max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
dev.off()

print(head(term_freq, 10))

k <- 5
lda_model <- LDA(dtm, k = k)
lda_terms <- terms(lda_model, 10)
print(lda_terms)
```

